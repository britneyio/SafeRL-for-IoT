{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225bf4f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b482fa7d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cyberbattle'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01miot_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CyberBattleIoT, is_unsafe_state\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcyberbattle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_dql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeepQLearnerPolicy\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcyberbattle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magents\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbaseline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01magent_wrapper\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train\n",
      "File \u001b[0;32m~/final_project/iot_env.py:5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"Model a Smart Home Environment \"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m\"\"\" See Jupyter notebook smarthome-simulation.ipynb for an example of game \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mplayed on this simulation\"\"\"\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcyberbattle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m model \u001b[38;5;28;01mas\u001b[39;00m m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcyberbattle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimulation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VulnerabilityInfo, FirewallConfiguration, FirewallRule, Identifiers, RulePermission\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict, Iterator, cast, Tuple\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cyberbattle'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from iot_env import CyberBattleIoT, is_unsafe_state\n",
    "from cyberbattle.agents.baseline.agent_dql import DeepQLearnerPolicy\n",
    "from cyberbattle.agents.baseline.agent_wrapper import train\n",
    "from project import SACAgent  # your custom SAC agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94842942",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 50\n",
    "MAX_STEPS = 1000\n",
    "SAFE_THRESHOLD = 0.5  # For SafeSAC filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b4031c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_unsafe_state(state):\n",
    "    # Basic heuristic: if state reveals access to a sensitive node, consider it unsafe\n",
    "    risky_keywords = [\"BabyMonitor\", \"DoorLock\", \"Thermostat\"]\n",
    "    state_str = str(state)\n",
    "    return any(keyword in state_str for keyword in risky_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3236066d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_dql():\n",
    "    print(\"Training DQL...\")\n",
    "    env = CyberBattleIoT()\n",
    "    agent = DeepQLearnerPolicy(\n",
    "        neural_net=None,\n",
    "        environment=env,\n",
    "        gamma=0.9,\n",
    "        learning_rate=0.01,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=20,\n",
    "    )\n",
    "    stats = train(env, agent, episodes=EPISODES)\n",
    "    return stats['rewards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d45d3ae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_sac():\n",
    "    print(\"Training SAC...\")\n",
    "    env = CyberBattleIoT()\n",
    "    agent = SACAgent(env)\n",
    "    rewards = []\n",
    "    for ep in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(MAX_STEPS):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.update_parameters()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        rewards.append(ep_reward)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910c9d95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_safe_sac():\n",
    "    print(\"Pretraining SAC for SafeSAC...\")\n",
    "    env = CyberBattleIoT()\n",
    "    agent = SACAgent(env)\n",
    "    q_safe = {}  # simulate safety critic\n",
    "    pretrain_rewards = []\n",
    "\n",
    "    # Pretraining Phase\n",
    "    for ep in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(MAX_STEPS):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            unsafe = is_unsafe_state(next_state)\n",
    "            q_safe[(str(state), action)] = 0.0 if unsafe else 1.0\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.update_parameters()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        pretrain_rewards.append(ep_reward)\n",
    "\n",
    "    print(\"Finetuning with SafeSAC constraints...\")\n",
    "    finetune_rewards = []\n",
    "    for ep in range(EPISODES):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(MAX_STEPS):\n",
    "            unsafe_actions = [a for a in range(env.action_space.n)\n",
    "                              if q_safe.get((str(state), a), 1.0) < SAFE_THRESHOLD]\n",
    "            action = agent.select_action(state)\n",
    "            while action in unsafe_actions:\n",
    "                action = agent.select_action(state)  # reject unsafe\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.update_parameters()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        finetune_rewards.append(ep_reward)\n",
    "\n",
    "    return pretrain_rewards, finetune_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d97a2d26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_results(dql, sac, safe_pre, safe_fine):\n",
    "    plt.figure()\n",
    "    plt.plot(dql, label=\"DQL\")\n",
    "    plt.plot(sac, label=\"SAC\")\n",
    "    plt.plot(safe_pre, label=\"SafeSAC Pretrain\")\n",
    "    plt.plot(safe_fine, label=\"SafeSAC Finetune\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Agent Reward Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"training_results.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc6d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    dql_rewards = run_dql()\n",
    "    sac_rewards = run_sac()\n",
    "    safe_pre, safe_fine = run_safe_sac()\n",
    "    plot_results(dql_rewards, sac_rewards, safe_pre, safe_fine)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
