{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5438d1ce",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dac8c806",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[1]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da02a0e6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from typing import NamedTuple, List, Optional, Tuple, Union, Dict\n",
    "from torch import Tensor\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "from cyberbattle.agents.baseline.learner import Learner\n",
    "from cyberbattle._env import cyberbattle_env\n",
    "from cyberbattle.agents.baseline.agent_wrapper import EnvironmentBounds\n",
    "import random\n",
    "from cyberbattle.agents.baseline.agent_dql import CyberBattleStateActionModel, random_argmax, ChosenActionMetadata, Transition, ReplayMemory\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7995762f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[2]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651d1fd6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class CategoricalPolicy(nn.Module):\n",
    "    def __init__(self, ep: EnvironmentBounds):\n",
    "        super(CategoricalPolicy, self).__init__()\n",
    "        model = CyberBattleStateActionModel(ep)\n",
    "        linear_input_size = len(model.state_space.dim_sizes)\n",
    "        output_size = model.action_space.flat_size()\n",
    "\n",
    "        self.hidden_layer1 = nn.Linear(linear_input_size, 1024)\n",
    "        # self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.hidden_layer2 = nn.Linear(1024, 512)\n",
    "        self.hidden_layer3 = nn.Linear(512, 128)\n",
    "        # self.hidden_layer4 = nn.Linear(128, 64)\n",
    "        self.head = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.hidden_layer3(x))\n",
    "        # x = F.relu(self.hidden_layer4(x))\n",
    "        return self.head(x.view(x.size(0), -1))\n",
    "    def act(self, states):\n",
    "        action_logits = self.head(states)\n",
    "        greedy_actions = torch.argmax(\n",
    "            action_logits, dim=1, keepdim=True)\n",
    "        return greedy_actions\n",
    "\n",
    "    def sample(self, states):\n",
    "        action_probs = F.softmax(self.head(states), dim=1)\n",
    "        action_dist = Categorical(action_probs)\n",
    "        actions = action_dist.sample().view(-1, 1)\n",
    "                # Avoid numerical instability.\n",
    "        z = (action_probs == 0.0).float() * 1e-8\n",
    "        log_action_probs = torch.log(action_probs + z)\n",
    "\n",
    "        return actions, action_probs, log_action_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2002b257",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[3]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd879607",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, ep: EnvironmentBounds):\n",
    "        super(Critic, self).__init__()\n",
    "        model = CyberBattleStateActionModel(ep)\n",
    "        linear_input_size = len(model.state_space.dim_sizes)\n",
    "        output_size = model.action_space.flat_size()\n",
    "\n",
    "        self.hidden_layer1 = nn.Linear(linear_input_size, 1024)\n",
    "        # self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.hidden_layer2 = nn.Linear(1024, 512)\n",
    "        self.hidden_layer3 = nn.Linear(512, 128)\n",
    "        # self.hidden_layer4 = nn.Linear(128, 64)\n",
    "        self.head = nn.Linear(128, output_size)\n",
    "\n",
    "    # Called with either one element to determine next action, or a batch\n",
    "    # during optimization. Returns tensor([[left0exp,right0exp]...]).\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.hidden_layer3(x))\n",
    "        # x = F.relu(self.hidden_layer4(x))\n",
    "        return self.head(x.view(x.size(0), -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42813893",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[7]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d15e63b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from cyberbattle.agents.baseline.agent_dql import CyberBattleStateActionModel\n",
    "from gymnasium.spaces.utils import flatten_space\n",
    "import numpy\n",
    "from cyberbattle._env.cyberbattle_env import Action\n",
    "class SACAgent(Learner):\n",
    "    def __init__(self, ep: EnvironmentBounds, gamma=0.99, tau=0.005, alpha=0.2,\n",
    "                 learning_rate=3e-4, replay_memory_size=1000000, batch_size=256):\n",
    "        self.stateaction_model = CyberBattleStateActionModel(ep)\n",
    "        self.gamma, self.tau, self.alpha = gamma, tau, alpha\n",
    "        self.batch_size = batch_size\n",
    "        self._last_actor_loss = 0.0\n",
    "        self._last_critic_loss = 0.0\n",
    "\n",
    "        self.actor = CategoricalPolicy(ep).to(device)\n",
    "        self.critic1 = Critic(ep).to(device)\n",
    "        self.critic2 = Critic(ep).to(device)\n",
    "        self.critic1_target = Critic(ep).to(device)\n",
    "        self.critic2_target = Critic(ep).to(device)\n",
    "        #hard update\n",
    "        self.critic1_target.load_state_dict(self.critic1.state_dict())\n",
    "        self.critic2_target.load_state_dict(self.critic2.state_dict())\n",
    "\n",
    "        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=learning_rate)\n",
    "        self.critic1_optimizer = optim.Adam(self.critic1.parameters(), lr=learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic2.parameters(), lr=learning_rate)\n",
    "\n",
    "        self.memory = ReplayMemory(replay_memory_size)\n",
    "\n",
    "\n",
    "\n",
    "    def parameters_as_string(self):\n",
    "        return f\"γ={self.gamma}, τ={self.tau}, α={self.alpha}, batch={self.batch_size}, memory={self.memory.capacity}\"\n",
    "    def all_parameters_as_string(self) -> str:\n",
    "        model = self.stateaction_model\n",
    "        return (\n",
    "            f\"{self.parameters_as_string()}\\n\"\n",
    "            f\"dimension={model.state_space.flat_size()}x{model.action_space.flat_size()}, \"\n",
    "            f\"Q={[f.name() for f in model.state_space.feature_selection]} \"\n",
    "            f\"-> 'abstract_action'\"\n",
    "        )\n",
    "    def _soft_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + param.data * self.tau)\n",
    "\n",
    "    def optimize_model(self, norm_clipping=False):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(self.batch_size)\n",
    "        # converts batch-array of Transitions to Transition of batch-arrays.\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map((lambda s: s is not None), batch.next_state)),\n",
    "            device=device,\n",
    "            dtype=torch.bool,\n",
    "        )\n",
    "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        reward_batch = torch.cat(batch.reward).to(device)\n",
    "        next_state_batch = torch.cat([s for s in batch.next_state if s is not None]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_probs = self.actor(next_state_batch)\n",
    "            min_next_q = torch.min(self.critic1_target(next_state_batch), self.critic2_target(next_state_batch))\n",
    "            next_value = (next_probs * (min_next_q - self.alpha * torch.log(next_probs + 1e-8))).sum(dim=1)\n",
    "            target_q = reward_batch + self.gamma * next_value\n",
    "\n",
    "        q1 = self.critic1(state_batch).gather(1, action_batch)\n",
    "        q2 = self.critic2(state_batch).gather(1, action_batch)\n",
    "\n",
    "        critic1_loss = F.mse_loss(q1, target_q.unsqueeze(1))\n",
    "        critic2_loss = F.mse_loss(q2, target_q.unsqueeze(1))\n",
    "\n",
    "        self.critic1_optimizer.zero_grad(); critic1_loss.backward(); self.critic1_optimizer.step()\n",
    "        self.critic2_optimizer.zero_grad(); critic2_loss.backward(); self.critic2_optimizer.step()\n",
    "\n",
    "        probs = self.actor(state_batch)\n",
    "        min_q = torch.min(self.critic1(state_batch), self.critic2(state_batch))\n",
    "        actor_loss = (probs * (self.alpha * torch.log(probs) - min_q)).sum(dim=1).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad(); actor_loss.backward(); self.actor_optimizer.step()\n",
    "        self._soft_update(self.critic1_target, self.critic1)\n",
    "        self._soft_update(self.critic2_target, self.critic2)\n",
    "\n",
    "        return self._last_actor_loss, critic1_loss.item(), critic2_loss.item()\n",
    "\n",
    "    def update_q_function(\n",
    "        self,\n",
    "        reward: float,\n",
    "        actor_state: ndarray,\n",
    "        abstract_action: np.int32,\n",
    "        next_actor_state: Optional[ndarray],\n",
    "    ):\n",
    "        reward_tensor = torch.tensor([reward], device=device, dtype=torch.float)\n",
    "        action_tensor = torch.tensor([[np.int_(abstract_action)]], device=device, dtype=torch.long)\n",
    "        current_state_tensor = torch.as_tensor(actor_state, dtype=torch.float, device=device).unsqueeze(0)\n",
    "        if next_actor_state is None:\n",
    "            next_state_tensor = None\n",
    "        else:\n",
    "            next_state_tensor = torch.as_tensor(next_actor_state, dtype=torch.float, device=device).unsqueeze(0)\n",
    "        self.memory.push(current_state_tensor, action_tensor, next_state_tensor, reward_tensor)\n",
    "\n",
    "        self.optimize_model()\n",
    "\n",
    "    def on_step(\n",
    "        self,\n",
    "        wrapped_env: w.AgentWrapper,\n",
    "        observation,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        truncated: bool,\n",
    "        info,\n",
    "        action_metadata,\n",
    "    ):\n",
    "        agent_state = wrapped_env.state\n",
    "        if done:\n",
    "            self.update_q_function(\n",
    "                reward,\n",
    "                actor_state=action_metadata.actor_state,\n",
    "                abstract_action=action_metadata.abstract_action,\n",
    "                next_actor_state=None,\n",
    "            )\n",
    "        else:\n",
    "            next_global_state = self.stateaction_model.global_features.get(agent_state, node=None)\n",
    "            next_actor_features = self.stateaction_model.node_specific_features.get(agent_state, action_metadata.actor_node)\n",
    "            next_actor_state = self.get_actor_state_vector(next_global_state, next_actor_features)\n",
    "\n",
    "            self.update_q_function(\n",
    "                reward,\n",
    "                actor_state=action_metadata.actor_state,\n",
    "                abstract_action=action_metadata.abstract_action,\n",
    "                next_actor_state=next_actor_state,\n",
    "            )\n",
    "\n",
    "\n",
    "    def metadata_from_gymaction(self, wrapped_env, gym_action):\n",
    "        current_global_state = self.stateaction_model.global_features.get(wrapped_env.state, node=None)\n",
    "        actor_node = cyberbattle_env.sourcenode_of_action(gym_action)\n",
    "        actor_features = self.stateaction_model.node_specific_features.get(wrapped_env.state, actor_node)\n",
    "        abstract_action = self.stateaction_model.action_space.abstract_from_gymaction(gym_action)\n",
    "        return ChosenActionMetadata(\n",
    "            abstract_action=abstract_action,\n",
    "            actor_node=actor_node,\n",
    "            actor_features=actor_features,\n",
    "            actor_state=self.get_actor_state_vector(current_global_state, actor_features),\n",
    "        )\n",
    "    def get_actor_state_vector(self, global_state: ndarray, actor_features: ndarray) -> ndarray:\n",
    "        return np.concatenate(\n",
    "            (\n",
    "                np.array(global_state, dtype=np.float32),\n",
    "                np.array(actor_features, dtype=np.float32),\n",
    "            )\n",
    "        )\n",
    "    def end_of_episode(self, i_episode, t):\n",
    "        # Update the target network, copying all weights and biases in DQN\n",
    "        if i_episode % t == 0:\n",
    "            self.critic1_target.load_state_dict(self.actor.state_dict())\n",
    "            self.critic2_target.load_state_dict(self.actor.state_dict())\n",
    "\n",
    "    def select_discrete_action(self, wrapped_env, device=\"cpu\", eval_mode=False):\n",
    "        \"\"\"\n",
    "        Flattens dict-based observation and samples action using a discrete policy.\n",
    "\n",
    "        Args:\n",
    "            policy_net (nn.Module): Your policy network (output is logits over discrete actions)\n",
    "            observation_space (gym.Space): The Dict observation space of the environment\n",
    "            observation_dict (dict): Actual observation returned from env\n",
    "            device (str): Device for torch tensor\n",
    "            eval_mode (bool): If True, selects greedy action; else samples stochastically\n",
    "\n",
    "        Returns:\n",
    "            action (int): The selected action index\n",
    "            probs (Tensor): Action probabilities\n",
    "            log_prob (Tensor): Log prob of selected action (None in eval mode)\n",
    "        \"\"\"\n",
    "        # Flatten dict observation into vector\n",
    "        flat_obs = flatten_space(wrapped_env.observation)\n",
    "        obs_tensor = torch.tensor(flat_obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "\n",
    "        # Get logits and sample\n",
    "        logits = self.actor(obs_tensor)\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        dist = Categorical(probs)\n",
    "\n",
    "        if eval_mode:\n",
    "            action = torch.argmax(probs, dim=-1)\n",
    "            log_prob = None\n",
    "        else:\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action.item(), probs, log_prob\n",
    "    def map_index_to_action(self, action_index: int,\n",
    "                        from_node: int,\n",
    "                        bounds,\n",
    "                        discovered_nodes: list[int],\n",
    "                        credential_cache_len: int) -> Action:\n",
    "        \"\"\"\n",
    "        Maps a discrete action index into a CyberBattleSim Action TypedDict.\n",
    "\n",
    "        Args:\n",
    "            action_index (int): The flat action index from the policy\n",
    "            from_node (int): The node initiating the action (must be owned)\n",
    "            bounds (EnvironmentBounds): The environment bounds\n",
    "            discovered_nodes (list[node_identifier]): External indices of discovered nodes\n",
    "            credential_cache_len (int): Number of discovered credentials\n",
    "\n",
    "        Returns:\n",
    "            Action (TypedDict): One of local_vulnerability / remote_vulnerability / connect\n",
    "        \"\"\"\n",
    "        local_total = bounds.local_attacks_count\n",
    "        remote_total = len(discovered_nodes) * bounds.remote_attacks_count\n",
    "        connect_total = len(discovered_nodes) * bounds.port_count * credential_cache_len\n",
    "\n",
    "        if action_index < local_total:\n",
    "            # Local vulnerability\n",
    "            vuln_index = action_index\n",
    "            return Action(local_vulnerability=numpy.array([from_node, vuln_index], dtype=numpy.int32))\n",
    "\n",
    "        elif action_index < local_total + remote_total:\n",
    "            # Remote vulnerability\n",
    "            offset = action_index - local_total\n",
    "            target_index = offset // bounds.remote_attacks_count\n",
    "            vuln_index = offset % bounds.remote_attacks_count\n",
    "            return Action(remote_vulnerability=numpy.array(\n",
    "                [from_node, target_index, vuln_index], dtype=numpy.int32))\n",
    "\n",
    "        else:\n",
    "            # Connect\n",
    "            offset = action_index - (local_total + remote_total)\n",
    "            product = bounds.port_count * credential_cache_len\n",
    "            target_index = offset // product\n",
    "            port_index = (offset % product) // credential_cache_len\n",
    "            cred_index = (offset % product) % credential_cache_len\n",
    "\n",
    "            return Action(connect=numpy.array(\n",
    "                [from_node, target_index, port_index, cred_index], dtype=numpy.int32))\n",
    "\n",
    "    def explore(self, wrapped_env: w.AgentWrapper) -> Tuple[str, cyberbattle_env.Action, object]:\n",
    "        \"\"\"Random exploration that avoids repeating actions previously taken in the same state\"\"\"\n",
    "        # sample local and remote actions only (excludes connect action)\n",
    "        gym_action, _, _ = self.select_discrete_action(wrapped_env=wrapped_env)\n",
    "        gym_action = self.map_index_to_action(gym_action, wrapped_env.state.node, wrapped_env.bounds, wrapped_env.observation[\"_discovered_nodes\"], len(w.owned_nodes(wrapped_env.observation)))\n",
    "        metadata = self.metadata_from_gymaction(wrapped_env, gym_action)\n",
    "        return \"explore\", gym_action, metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36ac8f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[5]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ac464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from final_project.iot_env import CyberBattleIoT\n",
    "import gymnasium as gym\n",
    "from typing import cast\n",
    "from cyberbattle.agents.baseline.agent_wrapper import AgentWrapper\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation\n",
    "env = cast(CyberBattleIoT, gym.make(\"CyberBattleIoT-v0\").unwrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81f581b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ep = w.EnvironmentBounds.of_identifiers(maximum_node_count=30, maximum_total_credentials=50, identifiers=env.identifiers)\n",
    "o, _ = env.reset()\n",
    "wrapped_env = AgentWrapper(env,ActionTrackingStateAugmentation(ep, o))\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "96c8b69a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "agent = SACAgent(\n",
    "    ep=ep,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.2,\n",
    "    learning_rate=3e-4,\n",
    "    replay_memory_size=10000,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dcda7df",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[6]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb9deffb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "feature only valid in the context of a node",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m observation, _ \u001b[38;5;241m=\u001b[39m wrapped_env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Always sample action from current stochastic policy (SAC exploration)\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     action_style, gym_action, metadata \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexplore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gym_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m  \u001b[38;5;66;03m# Skip if invalid action was returned\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 226\u001b[0m, in \u001b[0;36mSACAgent.explore\u001b[0;34m(self, wrapped_env)\u001b[0m\n\u001b[1;32m    223\u001b[0m current_global_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstateaction_model\u001b[38;5;241m.\u001b[39mstate_space\n\u001b[1;32m    225\u001b[0m \u001b[38;5;66;03m# Sample action from actor network\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m state_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstateaction_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwrapped_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    228\u001b[0m action_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor(state_tensor)\n\u001b[1;32m    229\u001b[0m action_dist \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdistributions\u001b[38;5;241m.\u001b[39mCategorical(action_probs)\n",
      "File \u001b[0;32m~/cyberbattle/agents/baseline/agent_wrapper.py:302\u001b[0m, in \u001b[0;36mConcatFeatures.get\u001b[0;34m(self, a, node)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, a: StateAugmentation, node: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the feature vector\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     feature_vector \u001b[38;5;241m=\u001b[39m [f\u001b[38;5;241m.\u001b[39mget(a, node) \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection]\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(feature_vector)\n",
      "File \u001b[0;32m~/cyberbattle/agents/baseline/agent_wrapper.py:302\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, a: StateAugmentation, node: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[1;32m    301\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the feature vector\"\"\"\u001b[39;00m\n\u001b[0;32m--> 302\u001b[0m     feature_vector \u001b[38;5;241m=\u001b[39m [\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_selection]\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mconcatenate(feature_vector)\n",
      "File \u001b[0;32m~/cyberbattle/agents/baseline/agent_wrapper.py:82\u001b[0m, in \u001b[0;36mNodeFeature.get\u001b[0;34m(self, a, node)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, a: StateAugmentation, node: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature only valid in the context of a node\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_at(a, node)\n",
      "\u001b[0;31mAssertionError\u001b[0m: feature only valid in the context of a node"
     ]
    }
   ],
   "source": [
    "total_numsteps = 0\n",
    "updates = 0\n",
    "total_reward = 0\n",
    "for i_episode in range(100):\n",
    "    episode_reward = 0\n",
    "    episode_steps = 0\n",
    "    done = False\n",
    "\n",
    "    observation, _ = wrapped_env.reset()\n",
    "\n",
    "    while not done:\n",
    "        # Always sample action from current stochastic policy (SAC exploration)\n",
    "        action_style, gym_action, metadata = agent.explore(wrapped_env)\n",
    "\n",
    "        if gym_action is None:\n",
    "            break  # Skip if invalid action was returned\n",
    "\n",
    "        # Take the action in the environment\n",
    "        next_observation, reward, done, truncated, info = wrapped_env.step(gym_action)\n",
    "        print(f\"Action: {gym_action}, Reward: {reward}, Done: {done}, Truncated: {truncated}, Info: {info}\")\n",
    "        # Store the transition and train agent if ready\n",
    "        agent.on_step(wrapped_env, next_observation, reward, done, truncated, info, metadata)\n",
    "\n",
    "        total_numsteps += 1\n",
    "        episode_steps += 1\n",
    "        episode_reward += reward\n",
    "        observation = next_observation\n",
    "\n",
    "    print(f\"Episode: {i_episode}, Total Steps: {total_numsteps}, Steps: {episode_steps}, Reward: {round(episode_reward, 2)}\")\n",
    "\n",
    "    # Evaluation every 10 episodes\n",
    "    if i_episode % 10 == 0:\n",
    "        avg_reward = 0.0\n",
    "        for _ in range(10):\n",
    "            observation, _ = wrapped_env.reset()\n",
    "            episode_reward = 0\n",
    "            done = False\n",
    "            while not done:\n",
    "                action_style, gym_action, metadata = agent.exploit(wrapped_env, observation)\n",
    "                if not gym_action:\n",
    "                    #stats[\"exploit_deflected_to_explore\"] += 1\n",
    "                    _, gym_action, action_metadata = learner.explore(wrapped_env)\n",
    "                if gym_action is None:\n",
    "                    break\n",
    "                observation, reward, done, truncated, info = env.step(gym_action)\n",
    "                episode_reward += reward\n",
    "            avg_reward += episode_reward\n",
    "        avg_reward /= 10\n",
    "        print(f\"[Eval] Episode {i_episode}, Avg Reward: {avg_reward:.2f}\")\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cc1a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[48]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "80840a0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SafetyCritic(nn.Module):\n",
    "    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(state_dim + action_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, state: Tensor, action: Tensor) -> Tensor:\n",
    "        # One-hot encode the action\n",
    "        action_onehot = F.one_hot(action.squeeze(-1), num_classes=self.fc3.out_features).float()\n",
    "        x = torch.cat([state, action_onehot], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x))  # returns value in [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daee87e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[44]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d90c2c33",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SafeSACPolicy(SACAgent):\n",
    "    def __init__(self, *args, safety_threshold: float = 0.5, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        state_dim = self.stateaction_model.state_space.flat_size()\n",
    "        action_dim = self.stateaction_model.action_space.flat_size()\n",
    "\n",
    "        self.safety_critic = SafetyCritic(state_dim, action_dim, hidden_dim=256).to(device)\n",
    "        self.safety_critic_optimizer = optim.Adam(self.safety_critic.parameters(), lr=3e-4)\n",
    "        self.safety_threshold = safety_threshold\n",
    "        self.in_finetune_phase = False\n",
    "\n",
    "    def enable_finetuning(self):\n",
    "        self.in_finetune_phase = True\n",
    "\n",
    "    def disable_finetuning(self):\n",
    "        self.in_finetune_phase = False\n",
    "\n",
    "    def update_safety_critic(self, batch: Transition, unsafe_labels: List[int]) -> float:\n",
    "        state_batch = torch.cat(batch.state).to(device)\n",
    "        action_batch = torch.cat(batch.action).to(device)\n",
    "        label_tensor = torch.tensor(unsafe_labels, dtype=torch.float, device=device).unsqueeze(1)\n",
    "\n",
    "        pred = self.safety_critic(state_batch, action_batch)\n",
    "        loss = F.binary_cross_entropy(pred, label_tensor)\n",
    "\n",
    "        self.safety_critic_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.safety_critic_optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def select_action(self, state: Tensor, evaluate: bool = False) -> Tuple[Tensor, Tensor, Optional[Tensor]]:\n",
    "        if not self.in_finetune_phase:\n",
    "            return super().select_action(state, evaluate)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            action_probs = self.actor(state)\n",
    "            all_actions = torch.arange(action_probs.shape[-1], device=device).unsqueeze(0).repeat(state.shape[0], 1)\n",
    "\n",
    "            safety_scores = self.safety_critic(state.repeat_interleave(action_probs.shape[-1], dim=0),\n",
    "                                               all_actions.view(-1, 1))\n",
    "            safety_scores = safety_scores.view(state.shape[0], -1)\n",
    "\n",
    "            mask = (safety_scores > self.safety_threshold).float()\n",
    "            filtered_probs = action_probs * mask\n",
    "            filtered_probs = filtered_probs / (filtered_probs.sum(dim=-1, keepdim=True) + 1e-8)\n",
    "\n",
    "            dist = Categorical(filtered_probs)\n",
    "            action = dist.sample()\n",
    "            log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action, filtered_probs, log_prob\n",
    "\n",
    "    def new_episode(self):\n",
    "        \"\"\"Called at the start of each new episode\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420f0769",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[45]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8861650b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def safe_sac_training(\n",
    "    cyberbattle_gym_env: cyberbattle_env.CyberBattleEnv,\n",
    "    environment_properties: EnvironmentBounds,\n",
    "    learner: 'SafeSACPolicy',\n",
    "    title: str,\n",
    "    pretrain_episodes: int,\n",
    "    finetune_episodes: int,\n",
    "    iteration_count: int,\n",
    "    initial_alpha: float = 0.2,\n",
    "    alpha_decay: float = 0.995,\n",
    "    min_alpha: float = 0.01,\n",
    "    render: bool = True,\n",
    "    verbosity: w.Verbosity = w.Verbosity.Normal,\n",
    "    label_unsafe_fn = None  # Custom unsafe labeling function\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    SafeSAC training loop with pretraining + finetuning phases.\n",
    "\n",
    "    Parameters\n",
    "    ==========\n",
    "    learner -- the SafeSACPolicy learner\n",
    "    label_unsafe_fn -- function that returns a list of 0 (unsafe) or 1 (safe) labels\n",
    "                      given (state, action, reward) transitions\n",
    "    \"\"\"\n",
    "    stats = {\n",
    "        'train_rewards': [],\n",
    "        'episode_lengths': [],\n",
    "        'actor_losses': [],\n",
    "        'critic_losses': [],\n",
    "        'alphas': []\n",
    "    }\n",
    "    initial_observation, info = cyberbattle_gym_env.reset()\n",
    "\n",
    "    # Initialize environment with wrapper\n",
    "    wrapped_env = w.AgentWrapper(\n",
    "        cyberbattle_gym_env,\n",
    "        w.ActionTrackingStateAugmentation(environment_properties,initial_observation)\n",
    "    )\n",
    "\n",
    "    def run_phase(phase_name, num_episodes, finetune=False):\n",
    "        if finetune:\n",
    "            learner.enable_finetuning()\n",
    "        else:\n",
    "            learner.disable_finetuning()\n",
    "\n",
    "        for episode in range(num_episodes):\n",
    "            learner.new_episode()\n",
    "            current_alpha = max(initial_alpha * (alpha_decay ** episode), min_alpha)\n",
    "            learner.alpha = current_alpha\n",
    "            total_reward = 0\n",
    "            steps = 0\n",
    "            transition_data = []\n",
    "\n",
    "            # Reset environment and get initial observation\n",
    "            observation, info = wrapped_env.reset()\n",
    "            if observation is None:\n",
    "                print(f\"Warning: Received None observation on reset in {phase_name} episode {episode}\")\n",
    "                continue\n",
    "\n",
    "            done = False\n",
    "            truncated = False\n",
    "\n",
    "            while not done and not truncated and steps < iteration_count:\n",
    "                action_name, action, metadata = learner.explore(wrapped_env)\n",
    "                if action is None:\n",
    "                    break\n",
    "\n",
    "                next_observation, reward, terminated, truncated, info = wrapped_env.step(action)\n",
    "                if next_observation is None:\n",
    "                    print(f\"Warning: Received None observation on step in {phase_name} episode {episode}\")\n",
    "                    break\n",
    "\n",
    "                done = terminated or truncated\n",
    "\n",
    "                # Store transition for safety critic\n",
    "                if not finetune and metadata is not None:\n",
    "                    transition_data.append((\n",
    "                        metadata.actor_state,\n",
    "                        metadata.abstract_action,\n",
    "                        reward\n",
    "                    ))\n",
    "\n",
    "                total_reward += reward\n",
    "                steps += 1\n",
    "                observation = next_observation\n",
    "\n",
    "            if not finetune and label_unsafe_fn is not None and transition_data:\n",
    "                unsafe_labels = label_unsafe_fn(transition_data)\n",
    "                states = [torch.tensor([s], dtype=torch.float32, device=device) for s, _, _ in transition_data]\n",
    "                actions = [torch.tensor([[a]], dtype=torch.long, device=device) for _, a, _ in transition_data]\n",
    "                rewards = [torch.tensor([r], dtype=torch.float32, device=device) for _, _, r in transition_data]\n",
    "\n",
    "                # Create valid next_states list with empty tensors for terminal states\n",
    "                next_states = [torch.zeros_like(states[0]) for _ in range(len(states))]\n",
    "\n",
    "                batch = Transition(states, actions, next_states, rewards)\n",
    "                safety_loss = learner.update_safety_critic(batch, unsafe_labels)\n",
    "                if verbosity != w.Verbosity.Quiet:\n",
    "                    print(f\"Safety critic loss: {safety_loss:.4f}\")\n",
    "\n",
    "            stats['train_rewards'].append(total_reward)\n",
    "            stats['episode_lengths'].append(steps)\n",
    "            stats['alphas'].append(current_alpha)\n",
    "            if hasattr(learner, 'last_actor_loss'):\n",
    "                stats['actor_losses'].append(learner.last_actor_loss)\n",
    "            if hasattr(learner, 'last_critic_loss'):\n",
    "                stats['critic_losses'].append(learner.last_critic_loss)\n",
    "\n",
    "            if verbosity != w.Verbosity.Quiet:\n",
    "                print(f\"{phase_name} Episode {episode + 1}/{num_episodes} - Steps: {steps}, Reward: {total_reward:.2f}, Alpha: {current_alpha:.3f}\")\n",
    "\n",
    "            if render:\n",
    "                wrapped_env.render()\n",
    "\n",
    "    print(\"\\n[Phase 1] Pretraining on base environment...\")\n",
    "    run_phase(\"Pretraining\", pretrain_episodes, finetune=False)\n",
    "\n",
    "    print(\"\\n[Phase 2] Finetuning on target task with safety constraints...\")\n",
    "    run_phase(\"Finetuning\", finetune_episodes, finetune=True)\n",
    "\n",
    "    return {\n",
    "        'learner': learner,\n",
    "        'stats': stats,\n",
    "        'title': title,\n",
    "        'trained_on': str(cyberbattle_gym_env),\n",
    "        'final_policy': learner.actor.state_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe890dcb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[49]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4dd4e77d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "agent = SafeSACPolicy(\n",
    "    ep=ep,\n",
    "    gamma=0.99,\n",
    "    tau=0.005,\n",
    "    alpha=0.2,\n",
    "    learning_rate=3e-4,\n",
    "    replay_memory_size=10000,\n",
    "    batch_size=64)\n",
    "def dummy_label_unsafe_fn(transitions):\n",
    "    # Example: mark as unsafe if reward < 0\n",
    "    return [0 if r >= 0 else 1 for _, _, r in transitions]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1315f04",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "In[50]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "0021a48f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Phase 1] Pretraining on base environment...\n",
      "Pretraining Episode 1/5 - Steps: 0, Reward: 0.00, Alpha: 0.200\n",
      "Pretraining Episode 2/5 - Steps: 0, Reward: 0.00, Alpha: 0.199\n",
      "Pretraining Episode 3/5 - Steps: 0, Reward: 0.00, Alpha: 0.198\n",
      "Pretraining Episode 4/5 - Steps: 0, Reward: 0.00, Alpha: 0.197\n",
      "Pretraining Episode 5/5 - Steps: 0, Reward: 0.00, Alpha: 0.196\n",
      "\n",
      "[Phase 2] Finetuning on target task with safety constraints...\n",
      "Finetuning Episode 1/5 - Steps: 0, Reward: 0.00, Alpha: 0.200\n",
      "Finetuning Episode 2/5 - Steps: 0, Reward: 0.00, Alpha: 0.199\n",
      "Finetuning Episode 3/5 - Steps: 0, Reward: 0.00, Alpha: 0.198\n",
      "Finetuning Episode 4/5 - Steps: 0, Reward: 0.00, Alpha: 0.197\n",
      "Finetuning Episode 5/5 - Steps: 0, Reward: 0.00, Alpha: 0.196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/lib/python3.10/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.is_action_valid to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.is_action_valid` for environment variables or `env.get_wrapper_attr('is_action_valid')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "results = safe_sac_training(\n",
    "    cyberbattle_gym_env=wrapped_env,\n",
    "    environment_properties=ep,\n",
    "    learner=agent,\n",
    "    title=\"SmartHomeSafeSAC\",\n",
    "    pretrain_episodes=5,\n",
    "    finetune_episodes=5,\n",
    "    iteration_count=30,\n",
    "    render=False,\n",
    "    label_unsafe_fn=dummy_label_unsafe_fn\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
