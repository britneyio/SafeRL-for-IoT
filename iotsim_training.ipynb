{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225bf4f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b482fa7d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys, os\n",
    "\n",
    "# Add the notebook's actual directory to sys.path\n",
    "\n",
    "from final_project.iot_env import CyberBattleIoT\n",
    "from cyberbattle.agents.baseline import agent_dql\n",
    "from cyberbattle.agents.baseline.agent_dql import DeepQLearnerPolicy\n",
    "#from cyberbattle.agents.baseline.agent_wrapper import train\n",
    "from iot_agents import DiscreteSACAgent, SafeSACAgent, ReplayBuffer  # your custom SAC agent\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from typing import cast\n",
    "from cyberbattle._env.cyberbattle_env import CyberBattleEnv\n",
    "from cyberbattle._env.flatten_wrapper import (\n",
    "    FlattenObservationWrapper,\n",
    "    FlattenActionWrapper,\n",
    ")\n",
    "from stable_baselines3.common.type_aliases import GymEnv\n",
    "from stable_baselines3.common.env_checker import check_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "94842942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "EPISODES = 50\n",
    "MAX_STEPS = 1000\n",
    "SAFE_THRESHOLD = 0.5  # For SafeSAC filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e9895-9b59-4e80-822c-8a70460a22e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9b4031c",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def is_unsafe_state(state):\n",
    "    # Basic heuristic: if state reveals access to a sensitive node, consider it unsafe\n",
    "    risky_keywords = [\"BabyMonitor\", \"DoorLock\", \"Thermostat\"]\n",
    "    state_str = str(state)\n",
    "    return any(keyword in state_str for keyword in risky_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3236066d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_dql(env, ep, title, episodes, steps, epsilon, gamma, learning_rate, batch_size, epsilon_decay, target_update,replay_memory_size):\n",
    "    print(\"Training DQL...\")\n",
    "    dqn = DeepQLearnerPolicy(\n",
    "            ep=ep,\n",
    "            gamma=gamma,\n",
    "            replay_memory_size=replay_memory_size,\n",
    "            target_update=target_update,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "    result = learner.epsilon_greedy_search(\n",
    "        cyberbattle_gym_env=env,\n",
    "        environment_properties=ep,\n",
    "        learner=dqn,\n",
    "        title=title,\n",
    "        episode_count=episodes,\n",
    "        iteration_count=steps,\n",
    "        epsilon=epsilon,\n",
    "        epsilon_exponential_decay=epsilon_decay,\n",
    "        epsilon_minimum=0.10,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        render=True,\n",
    "        plot_episodes_length=True\n",
    "    )\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03049a27-18c7-4749-96d2-519e1418000c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_state(obs_dict):\n",
    "    return np.concatenate([\n",
    "        np.atleast_1d(value).astype(np.float32)\n",
    "        for key, value in obs_dict.items()\n",
    "    ])\n",
    "\n",
    "def sac_training(env, episodes=1000, replay_size=10000, batch_size=64, **kwargs):\n",
    "    obs_space = 2 * env.bounds.maximum_node_count\n",
    "    act_space = env.bounds.maximum_node_count * 1 + env.bounds.local_attacks_count + env.bounds.remote_attacks_count\n",
    "\n",
    "    n_actions = act_space\n",
    "    agent = DiscreteSACAgent(obs_space, act_space, **kwargs)\n",
    "    memory = ReplayBuffer(replay_size)\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, _ = env.reset()\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = agent.act(flatten_state(state))\n",
    "            next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "\n",
    "            memory.push(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            if len(memory) > batch_size:\n",
    "                agent.update(memory, batch_size)\n",
    "\n",
    "        print(f\"Episode {ep}, Reward: {total_reward:.2f}\")\n",
    "\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537facc9-de9f-47a1-92e4-03e1e34dd030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "910c9d95",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def run_safe_sac(env, episodes, steps):\n",
    "    print(\"Pretraining SAC for SafeSAC...\")\n",
    "    env = CyberBattleIoT()\n",
    "    agent = SACAgent(env)\n",
    "    q_safe = {}  # simulate safety critic\n",
    "    pretrain_rewards = []\n",
    "\n",
    "    # Pretraining Phase\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        state = np.concatenate(list(obs.values()))\n",
    "        ep_reward = 0\n",
    "        for step in range(steps):\n",
    "            action = agent.select_action(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            unsafe = is_unsafe_state(next_state)\n",
    "            q_safe[(str(state), action)] = 0.0 if unsafe else 1.0\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.update_parameters()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        pretrain_rewards.append(ep_reward)\n",
    "\n",
    "    print(\"Finetuning with SafeSAC constraints...\")\n",
    "    finetune_rewards = []\n",
    "    for ep in range(episodes):\n",
    "        state = env.reset()\n",
    "        ep_reward = 0\n",
    "        for step in range(steps):\n",
    "            unsafe_actions = [a for a in range(env.action_space.n)\n",
    "                              if q_safe.get((str(state), a), 1.0) < SAFE_THRESHOLD]\n",
    "            action = agent.select_action(state)\n",
    "            while action in unsafe_actions:\n",
    "                action = agent.select_action(state)  # reject unsafe\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
    "            agent.update_parameters()\n",
    "            state = next_state\n",
    "            ep_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        finetune_rewards.append(ep_reward)\n",
    "\n",
    "    return pretrain_rewards, finetune_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d97a2d26",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_results(dql, sac, safe_pre, safe_fine):\n",
    "    plt.figure()\n",
    "    plt.plot(dql, label=\"DQL\")\n",
    "    plt.plot(sac, label=\"SAC\")\n",
    "    plt.plot(safe_pre, label=\"SafeSAC Pretrain\")\n",
    "    plt.plot(safe_fine, label=\"SafeSAC Finetune\")\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Agent Reward Comparison\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.savefig(\"training_results.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a8dc6d84",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtering out field _discovered_nodes\n",
      "Filtering out field _explored_network\n",
      "Filtering out field action_mask\n",
      "// MultiDiscrete flattened from [[3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]] -> [3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n",
      " 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      "// MultiDiscrete flattened from [4 4 4 4 4 4 4 4 4 4 4 4] -> [4 4 4 4 4 4 4 4 4 4 4 4]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/gymnasium/core.py:311: UserWarning: \u001b[33mWARN: env.bounds to get variables from other wrappers is deprecated and will be removed in v1.0, to get this variable you can do `env.unwrapped.bounds` for environment variables or `env.get_wrapper_attr('bounds')` that will search the reminding wrappers.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x299 and 24x512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 53\u001b[0m\n\u001b[1;32m     50\u001b[0m REPLAY_MEMORY_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10000\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# agent, rewards = train_discrete_sac(env_as_gym, episodes=10)\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m total_rewards, failure_counts, avg_returns \u001b[38;5;241m=\u001b[39m \u001b[43msac_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_as_gym\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepisodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreplay_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreplay_memory_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgamma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m#sac_rewards = run_sac(env_as_gym, episodes, steps)\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m#safe_pre, safe_fine = run_safe_sac(env_as_gym, episodes, steps)\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m#plot_results(dql_rewards, sac_rewards, safe_pre, safe_fine)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[17], line 21\u001b[0m, in \u001b[0;36msac_training\u001b[0;34m(env, episodes, replay_size, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 21\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflatten_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m     next_state, reward, terminated, truncated, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     23\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/final_project/iot_agents.py:588\u001b[0m, in \u001b[0;36mDiscreteSACAgent.act\u001b[0;34m(self, state, action_mask, eval)\u001b[0m\n\u001b[1;32m    585\u001b[0m     state \u001b[38;5;241m=\u001b[39m flatten_state(state)\n\u001b[1;32m    587\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 588\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m action_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m     mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(action_mask)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/final_project/iot_agents.py:447\u001b[0m, in \u001b[0;36mDiscretePolicy.forward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m--> 447\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    448\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[1;32m    449\u001b[0m     x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x))\n",
      "File \u001b[0;32m/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/miniconda/envs/cybersim/lib/python3.9/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x299 and 24x512)"
     ]
    }
   ],
   "source": [
    "iot_env = CyberBattleIoT(\n",
    "    maximum_node_count=12,\n",
    "    maximum_total_credentials=10,\n",
    "    observation_padding=True,\n",
    "    throws_on_invalid_actions=False,\n",
    ")\n",
    "# iot_env = cast(CyberBattleEnv, _gym_env)\n",
    "\n",
    "# ep = w.EnvironmentBounds.of_identifiers(maximum_node_count=12, maximum_total_credentials=10, identifiers=iot_env.identifiers)\n",
    "\n",
    "flatten_action_env = FlattenActionWrapper(iot_env)\n",
    "flatten_obs_env = FlattenObservationWrapper(flatten_action_env, ignore_fields=[\n",
    "    \"_credential_cache\",\n",
    "    \"_discovered_nodes\",\n",
    "    \"_explored_network\",\n",
    "    \"action_mask\"\n",
    "])\n",
    "\n",
    "episodes = 10\n",
    "steps = 100\n",
    "gamma = 0.9 # discount factor\n",
    "#safe_gamma = 0.99 # safe discount factor\n",
    "learning_rate = 1e-3\n",
    "batch_size = 100 # Deep Q-learning batch\n",
    "epsilon_decay = 0.01\n",
    "target_update = 5 #Deep Q-learning replay frequency (in number of episodes)\n",
    "replay_memory_size = 1000 # replay_memory_size -- size of the replay memory\n",
    "epsilon  =0.9\n",
    " # NOTE: Given the relatively low number of training episodes (50,\n",
    "# a high learning rate of .99 gives better result\n",
    "# than a lower learning rate of 0.25 (i.e. maximal rewards reached faster on average).\n",
    "# Ideally we should decay the learning rate just like gamma and train over a\n",
    "# much larger number of episodes\n",
    "\n",
    "\n",
    "\n",
    "env_as_gym = cast(GymEnv, flatten_obs_env)\n",
    "#check_env(flatten_obs_env)\n",
    "\n",
    "#dql_rewards = run_dql(env=iot_env, ep=ep, title=\"DQN in IoT Env\", episodes=episodes, steps=steps, epsilon=epsilon, gamma=gamma, learning_rate=learning_rate, batch_size=batch_size, epsilon_decay=epsilon_decay, target_update=target_update,replay_memory_size=replay_memory_size)\n",
    "#sac\n",
    "EPISODES = 10\n",
    "MAX_STEPS = 1000\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZE = 64\n",
    "EPSILON = 0.9\n",
    "EPSILON_DECAY = 5000\n",
    "TARGET_UPDATE = 5\n",
    "REPLAY_MEMORY_SIZE = 10000\n",
    "# agent, rewards = train_discrete_sac(env_as_gym, episodes=10)\n",
    "\n",
    "total_rewards, failure_counts, avg_returns = sac_training(env_as_gym, episodes=episodes, replay_size=replay_memory_size, gamma=gamma, alpha=0.2, lr=learning_rate,\n",
    "                 batch_size=batch_size)\n",
    "#sac_rewards = run_sac(env_as_gym, episodes, steps)\n",
    "#safe_pre, safe_fine = run_safe_sac(env_as_gym, episodes, steps)\n",
    "#plot_results(dql_rewards, sac_rewards, safe_pre, safe_fine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8518b66c-0b40-484d-b9a0-17d277c8acc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1782d25f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python [conda env:cybersim] *",
   "language": "python",
   "name": "conda-env-cybersim-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
